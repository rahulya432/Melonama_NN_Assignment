# Melanoma Skin Cancer Detection Group Project
Group project headed by Rahul ,Veera and Nikhit to identify Melanoma (Skin Cancer) using Convolutional Neural Network (CNN)

from a dataset of 2357 images of malignant and benign oncological diseases, which were formed from the International Skin Imaging Collaboration (ISIC)

Melanoma is the deadliest and most aggressive form of skin cancer; it is estimated that 7,650 deaths will be attributed to melanoma in the year 2022. However, if Melanoma is caught in an early stage, the 5-year survival rate is about 99%. Therefore, the early detection of Melanoma, before metastasis, is critical for patient survival.

# General Information

Problem statement: To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution that can evaluate images and alert dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis.

The dataset consists of 2357 images of malignant and benign oncological diseases, which were formed from the International Skin Imaging Collaboration (ISIC). All images were sorted according to the classification taken with ISIC, and all subsets were divided into the same number of images, with the exception of melanomas and moles, whose images are slightly dominant.

The data set contains the following diseases:

Actinic keratosis

Basal cell carcinoma

Dermatofibroma

Melanoma

Nevus

Pigmented benign keratosis

Seborrheic keratosis

Squamous cell carcinoma

Vascular lesion

# [Technologies Used]
The team used several technologies to build the CNN model, including TensorFlow, Keras, Numpy, Pandas, Augmentor, PIL, os, and Matplotlib.
- TensorFlow
- Keras 
- Numpy
- Pandas
- Augmentor
- PIL
- os
- Matplotlib

# CNN Architecture Design
To classify skin cancer using skin lesion images, the team built a custom CNN model with several layers, including Rescaling, Convolutional, Pooling, Dropout, Flatten, ReLU, and Softmax. The Rescaling layer rescaled the input to be in the [0,1] range. The Convolutional layer processed the convolutional operation, converting all the pixels in its receptive field into a single value. The Pooling layer reduced the dimensions of feature maps, summarizing the features present in a region of the feature map generated by a convolution layer. The Dropout layer randomly set input units to 0 with a frequency of rate at each step during training time, preventing overfitting. The Flatten layer converted the data into a 1-D array for input into the next layer, while the ReLU activation function overcame the vanishing gradient problem. The Softmax function was used as the activation function in the output layer of neural network models, predicting a multinomial probability distribution
The details of Architecture desgin:
- Rescalling Layer - To rescale an input in the [0, 255] range to be in the [0, 1] range.
- Convolutional Layer - Convolutional operation processed on the layer. A convolution converts all the pixels in its receptive field into a single value. 
- Pooling Layer - Pooling layers used to reduce the dimensions of feature maps. Hence, it reduces the number of parameters required to learn and computation performed in the network. The pooling layer summarizes the features present in a region of the feature map generated by a convolution layer.
- Dropout Layer - Is mainly included to prevent overfitting. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time
- Flatten Layer - converting the data into a 1-D array for input into the next layer. We flatten the output of the convolutional layers to create a single long feature vector and in turn is connected to the final model called a fully-connected layer.
- Activation Function(ReLU) - The rectified linear activation function or ReLU in short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better.
- Activation Function(Softmax) - Softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution. Advantage of using Softmax is the output probabilities range. The range will 0 to 1, and the sum of all the probabilities will be equal to one.

# Model Evaluation & Observations:
#We got imablanced dataset The team built four models, with the Base model showing a significant difference in training and validation accuracy. With more Dropout layers, Model-2's performance improved, and Model-3 further improved after data augmentation with TensorFlow. Finally, Model-4 included class imbalance treatment with the Augmentor library. The training accuracy improved to nearly 92%, and the validation accuracy was around 80%. While the team noted that more extensive models with more layers and epochs could be built, the current model helped treat overfitting to some extent.

-Details of Four models we have built:
1)Base model-1 which is base model trained on 20epochs and with less dropout layer which has huge difference in training and validation accuracy(Training accuracy 73% and validation accuracy is 50%) by this it seems that model is overfit.

2)Model-2 with more dropout layer the model performance is improved and training accuracy increase to 55% and validation set accuracy is 50%.(By adding dropout layer we were trying to avoid overfitting in the model)

3)Model-3 after augumenting the data with tensor flow  the training acccuracy is 45% and and the validation accuracy
43% 

4)Model-4 class imbalance treatment with augumentor library :
- The training accuracy seems to be nearly**~92%**.
- The validation accuracy is nearly**~80%**.
- Though the model accuracy has improved, the**class rebalance**has helped**treat the overfitting to some extent**.
- Much better models could be built or tried out using**more epochs and more layers**.
